---
types: article
tags: []
images: []
layout: article
title: CERTH participated in the ImageCLEF 2012 Flickr photo annotation task
date: '2012-08-16T15:18:30+03:00'
---
<p>&nbsp;</p><div>CERTH participated in ImageCLEF Photo Annotation and Retrieval 2012, and more specifically in the first subtask, namely Visual and Concept detection, annotation using Flickr photos.&nbsp;The objective of taking part in the competition was to evaluate two multimedia indexing approaches developed within <a href="http://www.socialsensor.eu/">SocialSensor</a> and to compare the performance of methods using different sets of image features.</div><div>&nbsp;</div><div>The first of the tested approaches constructs a similarity graph that includes both train and test images and trains concepts detectors using the graph Laplacian Eigenmaps (LE) as features. The second approach utilizes the concept of a &ldquo;same class&rdquo; model which takes as input the set of distances between the image to be annotated and a reference item representing a target concept, and predicts if the image belongs to the target concept.</div><div>&nbsp;</div><div><div>Five runs were submitted. Run 1 ranked 5th out of 17 text-based methods (MiAP) and 7th (GMiAP, F-ex). The performance of the visual-based run (#2) was close to the median performance, ranking 13th out of 28 visual-based ones (MiAP, GMiAP) and 10th (F-ex). Finally, the multimodal runs ranked above average compared to competition. For instance, Run 3 ranked 15th out of the 35 multimodal runs (MiAP, GMiAP) and 18th (F-ex). Detailed results are accessible at the official <a href="http://www.imageclef.org/2012/photo-flickr/annotation">task page</a>.</div></div><div><div id="cke_pastebin" style="position: absolute; left: -1000px; top: 132px; width: 1px; height: 1px; overflow: hidden; ">&nbsp;</div><div id="cke_pastebin" style="position: absolute; left: -1000px; top: 132px; width: 1px; height: 1px; overflow: hidden; ">Five runs were submitted. Run 1 ranked 5th out of 17 text-based methods (MiAP) and 7th (GMiAP, F-ex). The performance of the visual-based run (#2) was close to the median performance, ranking 13th out of 28 visual-based ones (MiAP, GMiAP) and 10th (F-ex). Finally, the multimodal runs ranked above average compared to competition. For instance, Run 3 ranked 15th out of the 35 multimodal runs (MiAP, GMiAP) and 18th (F-ex). Detailed results are accessible at the official task page.</div></div><div id="cke_pastebin" style="position: absolute; left: -1000px; top: 12px; width: 1px; height: 1px; overflow: hidden; ">Five runs were submitted. Run 1 ranked 5th out of 17 text-based methods (MiAP) and 7th (GMiAP, F-ex). The performance of the visual-based run (#2) was close to the median performance, ranking 13th out of 28 visual-based ones (MiAP, GMiAP) and 10th (F-ex). Finally, the multimodal runs ranked above average compared to competition. For instance, Run 3 ranked 15th out of the 35 multimodal runs (MiAP, GMiAP) and 18th (F-ex). Detailed results are accessible at the official task page.CERTH participated in ImageCLEF Photo Annotation and Retrieval 2012, and more specifically in the first subtask, namely Visual and Concept detection, annotation using Flickr photos.&nbsp;</div><div id="cke_pastebin" style="position: absolute; left: -1000px; top: 12px; width: 1px; height: 1px; overflow: hidden; ">&nbsp;</div><div id="cke_pastebin" style="position: absolute; left: -1000px; top: 12px; width: 1px; height: 1px; overflow: hidden; ">The objective of taking part in the competition was to evaluate two multimedia indexing approaches developed within SocialSensor and to compare the performance of methods using different sets of image features.</div><div id="cke_pastebin" style="position: absolute; left: -1000px; top: 12px; width: 1px; height: 1px; overflow: hidden; ">&nbsp;</div><div id="cke_pastebin" style="position: absolute; left: -1000px; top: 12px; width: 1px; height: 1px; overflow: hidden; ">The first of the tested approaches constructs a similarity graph that includes both train and test images and trains concepts detectors using the graph Laplacian Eigenmaps (LE) as features. The second approach utilizes the concept of a &ldquo;same class&rdquo; model which takes as input the set of distances between the image to be annotated and a reference item representing a target concept, and predicts if the image belongs to the target concept.</div><div id="cke_pastebin" style="position: absolute; left: -1000px; top: 12px; width: 1px; height: 1px; overflow: hidden; ">&nbsp;</div><div id="cke_pastebin" style="position: absolute; left: -1000px; top: 12px; width: 1px; height: 1px; overflow: hidden; ">Five runs were submitted. Run 1 ranked 5th out of 17 text-based methods (MiAP) and 7th (GMiAP, F-ex). The performance of the visual-based run (#2) was close to the median performance, ranking 13th out of 28 visual-based ones (MiAP, GMiAP) and 10th (F-ex). Finally, the multimodal runs ranked above average compared to competition. For instance, Run 3 ranked 15th out of the 35 multimodal runs (MiAP, GMiAP) and 18th (F-ex). Detailed results are accessible at the official task page.CERTH participated in ImageCLEF Photo Annotation and Retrieval 2012, and more specifically in the first subtask, namely Visual and Concept detection, annotation using Flickr photos.&nbsp;</div><div id="cke_pastebin" style="position: absolute; left: -1000px; top: 12px; width: 1px; height: 1px; overflow: hidden; ">&nbsp;</div><div id="cke_pastebin" style="position: absolute; left: -1000px; top: 12px; width: 1px; height: 1px; overflow: hidden; ">The objective of taking part in the competition was to evaluate two multimedia indexing approaches developed within SocialSensor and to compare the performance of methods using different sets of image features.</div><div id="cke_pastebin" style="position: absolute; left: -1000px; top: 12px; width: 1px; height: 1px; overflow: hidden; ">&nbsp;</div><div id="cke_pastebin" style="position: absolute; left: -1000px; top: 12px; width: 1px; height: 1px; overflow: hidden; ">The first of the tested approaches constructs a similarity graph that includes both train and test images and trains concepts detectors using the graph Laplacian Eigenmaps (LE) as features. The second approach utilizes the concept of a &ldquo;same class&rdquo; model which takes as input the set of distances between the image to be annotated and a reference item representing a target concept, and predicts if the image belongs to the target concept.</div><div id="cke_pastebin" style="position: absolute; left: -1000px; top: 12px; width: 1px; height: 1px; overflow: hidden; ">&nbsp;</div><div id="cke_pastebin" style="position: absolute; left: -1000px; top: 12px; width: 1px; height: 1px; overflow: hidden; ">Five runs were submitted. Run 1 ranked 5th out of 17 text-based methods (MiAP) and 7th (GMiAP, F-ex). The performance of the visual-based run (#2) was close to the median performance, ranking 13th out of 28 visual-based ones (MiAP, GMiAP) and 10th (F-ex). Finally, the multimodal runs ranked above average compared to competition. For instance, Run 3 ranked 15th out of the 35 multimodal runs (MiAP, GMiAP) and 18th (F-ex). Detailed results are accessible at the official task page.</div>
